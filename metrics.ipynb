{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "metrics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM2nysV++g/rp6dXEUX5bPA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chrismarkella/Kaggle-access-from-Google-Colab/blob/master/metrics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UY6sK_MCcHad",
        "colab_type": "text"
      },
      "source": [
        "##Metrics to measure how good are the predictions:\n",
        "- Accuracy\n",
        "- Precision\n",
        "- Recall/Sensitivity\n",
        "- F1 score\n",
        "- R2 score\n",
        "- ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YSP_3nNncBZB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "outputId": "c54cff4f-657f-41bd-f12f-adfdfa01a4fa"
      },
      "source": [
        "import sklearn.metrics as met\n",
        "\n",
        "for score_method in filter(lambda s: s.find('score') > 0, dir(met)):\n",
        "    print(score_method)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_scorer\n",
            "accuracy_score\n",
            "adjusted_mutual_info_score\n",
            "adjusted_rand_score\n",
            "average_precision_score\n",
            "balanced_accuracy_score\n",
            "brier_score_loss\n",
            "calinski_harabasz_score\n",
            "calinski_harabaz_score\n",
            "cohen_kappa_score\n",
            "completeness_score\n",
            "consensus_score\n",
            "davies_bouldin_score\n",
            "dcg_score\n",
            "explained_variance_score\n",
            "f1_score\n",
            "fbeta_score\n",
            "fowlkes_mallows_score\n",
            "get_scorer\n",
            "homogeneity_score\n",
            "jaccard_score\n",
            "jaccard_similarity_score\n",
            "label_ranking_average_precision_score\n",
            "make_scorer\n",
            "mutual_info_score\n",
            "ndcg_score\n",
            "normalized_mutual_info_score\n",
            "precision_recall_fscore_support\n",
            "precision_score\n",
            "r2_score\n",
            "recall_score\n",
            "roc_auc_score\n",
            "silhouette_score\n",
            "v_measure_score\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wznvTlvGdnLt",
        "colab_type": "text"
      },
      "source": [
        "###Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4rhrZ9OcdmP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import fetch_openml"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NUgexgXemZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist = fetch_openml(name='mnist_784', version=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaZxlEaMdwNt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "926811e0-bdc2-47c1-854c-e5b89068613a"
      },
      "source": [
        "X, y = mnist['data'], mnist['target']\n",
        "y = y.astype(np.uint8)\n",
        "\n",
        "TRAIN_SIZE = 60000\n",
        "\n",
        "x_train, x_test, y_train, y_test = X[:TRAIN_SIZE], X[TRAIN_SIZE:], y[:TRAIN_SIZE], y[TRAIN_SIZE:]\n",
        "for _set in (x_train, x_test, y_train, y_test):\n",
        "    print(_set.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n",
            "(60000,)\n",
            "(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KwZ51kRp_Uc",
        "colab_type": "text"
      },
      "source": [
        "###Binary Classifier\n",
        "- We classify the digits as `five` or `not five(0,1,2,3,4,6,7,8,9)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gkxq4uVffLa6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_5 = (y_train == 5)\n",
        "y_test_5 = (y_test == 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ovBfimoqQzE",
        "colab_type": "text"
      },
      "source": [
        "###Our `NeverFiveClassifier` always predict `not five`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5XjdU0DfckG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class NeverFiveClassifier(BaseEstimator):\n",
        "    def fit(self, X, y=None):\n",
        "        pass\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.zeros((len(X), 1), dtype=bool)\n",
        "\n",
        "never_five_clf = NeverFiveClassifier()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIIH0C4XgciD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import cross_val_predict\n",
        "\n",
        "y_train_pred = cross_val_predict(estimator=never_five_clf, X=x_train, y=y_train_5, cv=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6CFW4KXsbjf",
        "colab_type": "text"
      },
      "source": [
        "###Confusion matrix\n",
        "\n",
        "- `rows` are the `actual` values\n",
        "- `columns` are the `predicted` values\n",
        "- since this is a `binary classifier`, we have `only two classes`(`five` and `not five`)\n",
        "- `first` row and column are the `negative class`(`non five`)\n",
        "- `second` row and column are the `positive class`(`five`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kafI3noHfgUF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c0054724-f9ed-4b7d-df6a-46c736448c1b"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "confusion_matrix(y_true=y_train_5, y_pred=y_train_pred)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[54579,     0],\n",
              "       [ 5421,     0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t38vKesbtwQI",
        "colab_type": "text"
      },
      "source": [
        "###TP, TN, FP, FN\n",
        "- True Positive(`TP`): predicted positive and the actual value is positive\n",
        "- True Negative(`TN`): predicted negative and the actual value is negative\n",
        "- False Positive(`FP`): predicted positive but the actual value is negative\n",
        "- False Negative(`FN`): predicted negative and the actual value is positive\n",
        "---\n",
        "- TN = 54579\n",
        "- TP = 0\n",
        "- FP = 0\n",
        "- FN = 5421"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJC7TKHqmN6",
        "colab_type": "text"
      },
      "source": [
        "###Our accuracy is close to `91 percent`, but in reality we just say `not five`.\n",
        "This is actually the right predictions most of the time, since only about 10 percent of the digits are `fives`.\n",
        "\n",
        "However we `never predicted any fives`.\n",
        "`precision` and `recall` is a better `metric` here. They are both show `zero` scores."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWcCrjEvgQ2j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "886431e2-cdd1-4bb3-8601-d8e59bb15bb9"
      },
      "source": [
        "from sklearn.metrics import (accuracy_score, precision_score,\n",
        "                             recall_score, f1_score) \n",
        "\n",
        "metric_functions = (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,)\n",
        "\n",
        "for func in metric_functions:\n",
        "    score = func(y_true=y_train_5, y_pred=y_train_pred)\n",
        "    print(f'{func.__name__:20}: {score}')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy_score      : 0.90965\n",
            "precision_score     : 0.0\n",
            "recall_score        : 0.0\n",
            "f1_score            : 0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}